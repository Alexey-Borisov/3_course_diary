# Дневник по научной работе у Дьяконова А.Г.

Тема: Умный поиск (по сайту). Мини-проект: есть неформальный запрос, найти наиболее релевантную информацию в источнике.



Начал с чтения книги "Хобсон, Ханнес, Коул: Обработка естественного языка в действии":

<img src="https://user-images.githubusercontent.com/72136589/139948459-4e404531-92a7-49dc-bd9b-54ef35f631dd.png" width="195" height="92*3">

Книга годится только для поверхностного обзора, никакой математики нет.

Продолжаю восполнять свои знания по теме языковых моделей и трансформеров. 

Нашел с виду интересную новую статью, с которой стоит ознакомиться: MNet-Sim: A Multi-layered Semantic Similarity
Network to Evaluate Sentence Similarity (https://arxiv.org/ftp/arxiv/papers/2111/2111.05412.pdf)

Еще одна статья, с которой хочу ознакомиться: Sentence-T5: Scalable Sentence Encoders
from Pre-trained Text-to-Text Models (https://arxiv.org/pdf/2108.08877.pdf)

Также в скором времени ознакомлюсь со статьей о Poly-encoders (https://arxiv.org/pdf/1905.01969.pdf)


Ознакомился со статьей о Poly-encoders, перед этим возобновив свои знания о Bi- и Cross-encoders. 

К сожалению, авторы не предоставили свои предобученные модели, поэтому использовать подход использующий Poly-encoders,
скорее всего не смогу. В ближайшее время собираюсь попробовать использовать Bi- и Cross-encoders используя предобученные модели 
из библиотеки SentenceTransformers (https://www.sbert.net). Также нужно будет прочитать статью авторов библиотеки SentenceTransformers - 
Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks (https://arxiv.org/pdf/1908.10084.pdf)

<img src="https://raw.githubusercontent.com/Alexey-Borisov/3_course_diary/main/source/encoders.png">

Вообще говоря Cross-encoders показывают лучшее качество чем Bi-encoders, и Poly-encoders позиционируется как архитектура способная дать лучшее качество
чем Bi-encoders и при этом оставить разумное время инференса. Большое время работы Cross-encoders обусловлено тем, что нам нужно для каждой пары 
контекст+кандидат находить эмбеддинги во время инференса, что дает квадратичную сложность и большие временные затраты.
А в случае Bi-encoders мы можем заранее посчитать все эмбеддинги кандидатов и во время инференса остается только получить
эмбеддинг контекста и посчитать функцию близости.
Но в нашей задаче у нас кандидатами являются предложения на сайте, и заранее посчитать эмбеддинги всех предложений
со всех сайтов не представляется возможным, поэтому основное преимущество использования Bi- и Poly-encoders пропадает.
Хотя при нескольких последовательных запросах при работе с одним сайтом, использование Bi- и Poly-encoders позволит сохранить
эмбеддинги предложений и ускорит время ответа на последующие запросы. 


