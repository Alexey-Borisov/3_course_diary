# Дневник по научной работе у Дьяконова А.Г. (активный раздел)

Тема: Metric Learning
___

## 14.02.2022:

Начал со следующей обзорной статьи, после чего было решено сосредоточиться на классическом metric learning без использования нейросетей.
- [x] Mahmut Kaya H.s. Bilge «Deep Metric Learning: A Survey» // August 2019Symmetry 11(9):1066 DOI: 10.3390/sym11091066 https://www.researchgate.net/publication/335314481_Deep_Metric_Learning_A_Survey

Далее ознакомлюсь со следующим туториалом:
- [x] Suárez J. L., García S., Herrera F. A Tutorial on Distance Metric Learning: Mathematical Foundations, Algorithms, Experimental Analysis, Prospects and Challenges //arXiv preprint arXiv:1812.05944. – 2020. https://arxiv.org/abs/1812.05944

___

## 25.02.2022:

Закончил с туториалом, подробно разобрался с работой некоторых алгоритмов Metric Learning:

1) Алгоритмы понижения размерностей: PCA, LDA, ANMM
2) Алгоритмы для улучшения работы классификатора ближайших соседей: LMNN, NCA, NCMML и NCMC
3) Алгоритмы, основанные на теории информации: ITML, DMLMJ, MCML
4) Некоторые другоие методы Metric Learning: LSI (Learning with Side Information), DML-eig, LDML
5) Ядерные обобщения некоторых алгоритмов: KLMNN, KANMM, KDMLMJ, KDA

Список ниже будет дополняться статьями, которые я буду изучать:
- [x] Nguyen, Bac; Morell, Carlos; De Baets, Bernard (2018). Scalable Large-Margin Distance Metric Learning Using Stochastic Gradient Descent. IEEE Transactions on Cybernetics, (), 1–12. doi:10.1109/TCYB.2018.2881417 
- [ ] H. Ye, D. Zhan, N. Li and Y. Jiang, "Learning Multiple Local Metrics: Global Consideration Helps" in IEEE Transactions on Pattern Analysis & Machine Intelligence, vol. 42, no. 07, pp. 1698-1712, 2020. doi: 10.1109/TPAMI.2019.2901675 (пока нигде не нашел бесплатно, но abstract звучит интересно)
- [ ] Bac Nguyen, Francesc J. Ferri, Carlos Morell, Bernard De Baets, An efficient method for clustered multi-metric learning, Information Sciences, Volume 471, 2019, Pages 149-163, ISSN 0020-0255, https://doi.org/10.1016/j.ins.2018.08.055.
- [x] B. Tang and H. He, "ENN: Extended Nearest Neighbor Method for Pattern Recognition [Research Frontier]," in IEEE Computational Intelligence Magazine, vol. 10, no. 3, pp. 52-60, Aug. 2015, doi: 10.1109/MCI.2015.2437512.
- [x] Bo Tang, Haibo He, Song Zhang, MCENN: A variant of extended nearest neighbor method for pattern recognition, Pattern Recognition Letters, Volume 133, 2020, Pages 116-122, ISSN 0167-8655, https://doi.org/10.1016/j.patrec.2020.01.015.
- [x] Chakraborty, Tapabrata (Rohan) et al. “Distance Metric Learned Collaborative Representation Classifier.” ArXiv abs/1905.01168 (2019): n. pag.
- [x] L. Feng, H. Wang, B. Jin, H. Li, M. Xue and L. Wang, "Learning a Distance Metric by Balancing KL-Divergence for Imbalanced Datasets," in IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 49, no. 12, pp. 2384-2395, Dec. 2019, doi: 10.1109/TSMC.2018.2790914.
- [x] Chen S. et al. Curvilinear Distance Metric Learning // Advances in Neural Information Processing Systems. – 2019. – С. 4225-4234. http://papers.nips.cc/paper/8675-curvilinear-distance-metric-learning
- [ ] [Metric Learning for Dynamic Text Classification](https://aclanthology.org/D19-6116) (Wohlwend et al., EMNLP 2019)
- [ ] Huang, Zhiwu et al. “Cross Euclidean-to-Riemannian Metric Learning with Application to Face Recognition from Video.” IEEE Transactions on Pattern Analysis and Machine Intelligence 40 (2018): 2827-2840.
- [x] Chen, Shuo et al. “Adversarial Metric Learning.” ArXiv abs/1802.03170 (2018): n. pag.
- [ ] Jie Xu, Lei Luo, Cheng Deng, and Heng Huang. Bilevel distance metric learning for robust image recognition. In NeurIPS, 2018. 1, 2.1
- [x] Han-Jia Ye, De-Chuan Zhan, Xue-Min Si, Yuan Jiang, and Zhi-Hua Zhou. What makes objects similar: a unified multi-metric learning approach. In NeurIPS, 2016. 1, 2.1
- [x] Pourya Zadeh, Reshad Hosseini, and Suvrit Sra. Geometric mean metric learning. In ICML, 2016. 1, 4.2
- [ ] Pengfei Zhu, Hao Cheng, Qinghua Hu, Qilong Wang, and Changqing Zhang. Towards generalized and efficient metric learning on riemannian manifold. In IJCAI, 2018. 1

___

Здесь будет выстраиваться некоторое распределение статей по темам, на которое будет опираться структура обзорной статьи.


+ Применение современных подходов к metric learning:
  1) Adversarial Metric Learning - подход, похожий на GAN. пытаемся искусственно создавать пары, на которых алгоритм ошибается.
  2) A Unified Framework for Metric Transfer Learning - transfer learning для metrcic learning
  3) Robust Transfer Metric Learning for Image Classification - аналогично


+ Методы решающие некоторые проблемы существующих алгоритмов:
  1) Scalable Large-Margin Distance Metric Learning Using Stochastic Gradient Descent - эффективный метод оптимизации, подходит для больших данных.
  2) Learning a Distance Metric by Balancing KL-Divergence for Imbalanced Datasets - для несбалансированных датасетов.

3) Подходы использующие оптмизацию на многообразии Римана или Грассмана, а также в пространстве неотрицательно опредеделенных матриц. (Riemannian manifold, Grassman manifold, PSD cone):
  * Towards generalized and efficient metric learning on riemannian manifold
  * Geometric mean metric learning
  * Cross Euclidean-to-Riemannian Metric Learning with Application to Face Recognition from Video


4) Нелинейные методы metric learning, можно подразделить на local metric learning, instance metric learning, multi-metric learning:
  * What makes objects similar: a unified multi-metric learning approach

5) Новые методы, связанные с metric learning:
  * MCENN: A variant of extended nearest neighbor method for pattern recognition
  * Distance Metric Learned Collaborative Representation Classifier
  * Supervised distance metric learning through maximization of the Jeffrey divergence


___

## 01.03.2022

Scalable Large-Margin Distance Metric Learning Using Stochastic Gradient Descent.

Основная идея та же, что у алгоритма LMNN (Large Margin Nearest Neighbors) &mdash; максимизировать некоторые отступы. В функцию потерь добавляют регуляризационное слагаемое trace(M) (ядерная норма M), которое штрафует за большой ранг матрицы M. Но основная часть статьи посвящена построению эффективного алгоритма для обучения с использованием SGD. С точки зрения точности представленная модель не может похвастаться значительным улучшением качества, однако достигнуто очень большое уменьшение времени обучения. Основная сложность оптимизации подобных функций потерь заключается в том, что на каждой итерации необходимо производить проекцию решения на множество положительно определенных матриц (PSD cone), для чего необходимо находить собственные значения матрицы. Для решения этой проблемы авторы на каждой итерации градиентного спуска делают шаг, который не выводит решение из множества положительно определенных матриц. Для этого были доказаны соответствующие теоремы.

___

## 06.03.2022

ENN: Extended Nearest Neighbor Method for Pattern Recognition

MCENN: A variant of extended nearest neighbor method for pattern
recognition

Две статьи, в первой из которых представлен новый метрический алгоритм, некоторое обобщение классического KNN. Это алгоритм в отличии от классического KNN при определении класса для нового объекта учитывает не только какие объекты являются его ближайшими соседями, но и для каких объектов обучающей выборки новый объект явдяется ближайшим соседом. Таким образом лучше учитывается распределение объектов и в некоторых случаях повышается качество предсказания. 

В второй статье предлагается метод для обучения расстояния, использование которого должно улучшить качество работы ENN. Как обычно обучается расстояние Махаланобиса, то есть линейное преобразование переводящее олбъекты в новое пространство. Использовался тот же подход, что и в методе NCA (Nearest Component Analysis): вводится вероятность правильной классификации каждого объекта обучающей выборки по остальной части и максимизируется количество верно классифицированных объектов, то есть напрямую оптимизируется LOO-ошибка. Как и для NCA, задача получается невыпуклая и возможно попадание в локальные минимумы при оптимизации градиентным спуском. Одним из недостатков NCA была высокая вероятность переобучения из-за прямой оптимизации LOO-ошибки, так что можно предположть такой же эффект и для MCENN алгоритма, но в статье данный вопрос не обсуждается.

___

## 09.03.2022

Learning a Distance Metric by Balancing KL-Divergence for Imbalanced Datasets

Статья поднимает проблему использования методов metric learning для несбалансированных данных. Авторы предоставляют новый метод metric learning, который способен учитывать дисбаланс классов и достигать лучшего качества чем друге методы на датасетах, в которых присутствует дисбаланс классов (оценивались accuracy, precision, recall, F-score). 
Вводится нормированная KL-дивергенция между различными классами после применения линейного преобразования (которое соответствует использованию новой метрики в исходном пространстве) после чего в качестве функционала качества используется геометрическое среднее этих KL-дивергенций для всех пар классов. Метод делает предположение, что каждый класс имеет нормальное распределение с некоторым матожиданием и ковариационной матрицей (могут быть различными для разных классов). Геометрическое среднее используется, для того чтобы метод старался одинаково хорошо "раздвигать" разные пары классов. Для лучшей дискриминативной способности добавляется слагаемое, штрафующее за расстояние между объектами разных классов. Это слагаемое берется с коэффициентом, который задает trade-off между геометрическим средним и максимизацией отступов. Также добавляются ограничения на матрицу задающую расстояние Махаланобиса: она должна быть положительно полуопределенной. Также ограничивается сумма расстояний между объектами одного класса. Обучение осуществляется с помощью градиентного спуска с проекциями на каждой итерации. То есть после каждого шага осуществляются проекции, для того чтобы текущее решение удовлетворяло ограничениям. На несбалансированных датасетах представленный метод показывает лучшие результаты чем LMNN, ITML и некоторые другие. В экспериментах использовался 1NN классификатор.

___

## 10.03.2022

Distance Metric Learned Collaborative Representation Classifier

В статье рассказывается о том, как использоввание metric learning может улучшить работу CRC. Сперва опишу работу классического CRC. Предполагается, что имеется некоторая нейросеть, которая находит хорошее представление объекта, например с помощью сверточной нейронной сети получаем эмбеддинги картинок. В этом методе объект, который необходимо классифицировать, представляют в виде линейной комбинации всех объектов обучающей выборки, решая задачу линейной регрессии c регуляризацией. Далее для каждого из классов, представленных в выборке, считается некоторый функционал, который показывает насколько сильно при построении представления нового объекта в виде линейной комбинации всех объектов участвовали объекты каждого класса. Таким образом выбирается класс, который был важнее всех при конструировании представления объекта в виде линейной комбинации.

В классическом варианте при решении задачи линейной регрессии минимизируется квадрат l2-нормы с l2-регуляризацией. Авторы предлагают более общий подход: использовать метрику Махаланобиса, а также добавить в качестве дополнительного регуляризующего слагаемого спектральную норму матрицы обратной матрице из метрики Махаланобиса (если в метрике S^(-1), то в слагаемом спектральная норма S). В классической постановке получалась простая задача, для которой можно явно записать точное решение (хотя для его нахождения требуется обращать матрицу, размер которой растет с размером выборки). Для новой, получившейся задачи авторы представляют итерационный алгоритм, в котором поочередно оптимизируется матрица, задающая расстояние Махаланобиса и кодирующий вектор, задающий коэффициенты линейной комбинации объектов обучающей выборки.

Авторы показывали эффективность своей модели на 3 датасетах детализированных изображений с сетью VGG-19, предобученной на датасете ImageNet. Для сравнения использовались несколько других моделей, как использующих CRC подход, так и другие сети для классификации детализированных изображений. Эксперименты показали, что представленный подход, позволяет достигать заметно лучшего качества на всех 3 представленных датасетах.  

___ 

## 15.03.2022

Geometric mean metric learning

Очередной метод metric learning, который однако довольно сильно отличается от других. Как и многие более ранние методы здесь авторы минимизируют расстояния между "похожими" объектами, но вместо того чтобы максимизировать расстояния между непохожими объектами, используя ту же метрику, порожденную матрицей A, которую и обучает метод, минимизируют расстояния между различными объектами в метрике, порожденной матрицей A^(-1). Это решение объясняется некоторыми фактами из линейной алгебры о положительно определенных матрицах. Минимизация получившегося функционала качества сводится к решению уравнения Риккати, точное решение которого можно получить. В статье показывают, что решение представляет собой середину геодезической кривой в пространстве симметричных положительно определенных матриц, соединяющей матрицы S^(-1) и D, которые представляют собой матрицы ковариации для похожих и непохожих объектов. Также решение можно параметризовать, рассматривая всю кривую, таким образом можно балансировать влияние похожих и непохожих объектов на решение. Также естественным образом добавляется регуляризация в случае вырожденности матрицы S. По сути решение задачи сводится к нахождению значения в определенных точках кривой в пространстве положительно определенных матриц, поэтому алгоритм работает заметно быстрее чем LMNN и ITML. В плане качества также во многих экспериментах метод показывал лучшие результаты чем LMNN, однако были показаны и случаи, когда он проигрывал ему в качестве.

___

## 30.03.2022 

Возвращаюсь к работе после длительного перерыва, связанного с большим количеством работ по другим дисциплинам.

Curvilinear Distance Metric Learning

Очень понравилась статья, авторы дают новый взгляд на работу алгоритмов metric learning, в частности на обучение метрики Махаланобиса, и объясняют почему эти методы не всегда могут хорошо приближать реальную геометрию исходного пространства. В статье упоминаются многие современные статьи, часть которых я уже прорабортал, а другие только предстоит. В новом методе, который представляют авторы, показывают как можно хорошо учитывать сложную геометрию исходных признаковых пространств, используя нелинейные функции. Предоставляется способ каким образом обучать эти функции, в статье авторы используют полиномиальные функции степени не выше "с" и ссылаются на Теорему Вейерштрасса о приближении непрерывной функции полиномами. Но также указывается, что можно использовать приближение с помощью рядов Фурье или кусочно-линейных функций. Задача сводится к минимизации невыпуклого функционала, обучается тензор, который содержит коэффициенты соответствующих полиномиальных функций. На каждой итерации оптимизации с использованием SGD необходимо находить calibration points для текущей пары объектов, для чего необходимо находить корень нелинейного уравнения. Авторы предлагают использовать для этого эффективные численные методы (но этот момент все равно кажется мне очень тяжеловесным). Метод показал очень хорошие результаты на синтетических данных (что логично, он ведь напрямую подстраивается под нелинейную природу данных). На реальных датасетах метод также показал хорошие результаты, однако часто дисперсия качества получалась довольно высокой. 
  
  Думаю над тем, чтобы после написания обзора подробно заняться этим методом, реализовать некоторый его вариант и провести эксперименты. Здесь присутствует большая свобода для выбора параметризации обучаемых кривых, итогового функционала качества, регуляризации.
  
  ___
  
  Adversarial Metric Learning
  
  В статье предоставляют метод, который позволяет бороться с проблемой того, что тестовые данные несколько отличаются от обучающих. Рассказывается о методе обучения в metric learning, в котором искусственно генерируются примеры, на котрых сеть ошибается и которые довольно близки к парам объектов из обучающих данных. Таким образом Метод на каждой итерации учится справляться со сложными примерами. Поочередно выполняются два шага: 1) Получение проблемных примеров (adversarial pairs) и 2) обновление метрики, чтобы она лучше с ними справлялась. Авторы предоставляют алгоритм, который позволяет избавиться от этой наивной двухэтапной реализации и получают формулу для шага градиентного спуска. Итоговый алгоритм представляет собой итерационный метод, на каждом шаге которого происходит обновление матрицы M, задающей метрику, и нахождение ее проекции на множество положительно определенных матриц (PSD cone). На синтетических данных показывается эффективность данного метода. На реальных датасетах метод также показал хорошее качество, зачастую превосходящее другие методы. В экспериментах в качестве базовой функции потерь использовалась та, что используется в методе Geometric Mean Metric Learning. Именно для этой функции потерь выводится явная формула шага итерационного процесса и доказывается теоретическое обоснование метода, например то, что во время итерационного шага матрица M не теряет свойство симметричности, что позволяет осуществить проекцию на PSD cone.
  
  ___
  
## 31.03.2022 
  
  What makes objects similar: a unified multi-metric learning approach
  
  
  В статье поднимается проблема того, что классические metric learning подходы, которые обучают одну глобальную метрику не могут учесть разнообразную природу данных и связей между ними и учитывают только пространственные связи. В свою очередь авторы предлагают подход, в котором обучается сразу несколько различных метрик, каждая из которых используется для получения "схожести" объектов (подразумевается, что каждая метрика может отражать похожесть объектов с точки зрения разных признаков). Результаты всех метрик некоторым обрахом агрегируются и используются в функции потерь с возможным регуляризатором. Присутствует очень большая вариативность: способ агрегации оценок "схожести" объектов в разных метриках, базовая функция потерь, регуляризатор, также можно обучать алгоритм используя как пары объектов, так и триплеты (как в LMNN). Как побочный эффект, данный метод позволяет извлекать признаки, так как каждая из обучаемых метрик может в конечном счете "отвечать" за схожесть объектов в определенном смысле. В экспериментах метод показал хорошие результаты, однако сравнивался он с относительно старыми алгоритмами, несмотря на то, что статья 2018 года. Хотел бы почитать их следующую статью, которая продолжает эту же тему, но не нашел нигде без подписки на журнал.
  
  ___
  
